{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/officialaa/final-ipynb?scriptVersionId=205791855\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-07T14:22:01.882007Z","iopub.execute_input":"2024-11-07T14:22:01.882856Z","iopub.status.idle":"2024-11-07T14:22:01.888757Z","shell.execute_reply.started":"2024-11-07T14:22:01.882823Z","shell.execute_reply":"2024-11-07T14:22:01.887754Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\n\nurl=\"https://raw.githubusercontent.com/sap03rocks/mimic_lstm/refs/heads/main/training70set.csv\"\n# Read the data into a DataFrame\n\ndata = pd.read_csv(url, header=None)\ndata = data.drop(columns=[0,1])\n\ndata","metadata":{"execution":{"iopub.status.busy":"2024-11-07T14:22:01.890575Z","iopub.execute_input":"2024-11-07T14:22:01.890992Z","iopub.status.idle":"2024-11-07T14:22:02.087394Z","shell.execute_reply.started":"2024-11-07T14:22:01.890962Z","shell.execute_reply":"2024-11-07T14:22:02.086464Z"},"trusted":true},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                2\n0       noise_ppg\n1     0.639026637\n2     0.538858406\n3     0.592074814\n4      0.64529059\n...           ...\n4997  0.347910535\n4998  0.185285058\n4999  0.197442822\n5000  0.202168183\n5001  0.207971804\n\n[5002 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>noise_ppg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.639026637</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.538858406</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.592074814</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.64529059</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>0.347910535</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>0.185285058</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>0.197442822</td>\n    </tr>\n    <tr>\n      <th>5000</th>\n      <td>0.202168183</td>\n    </tr>\n    <tr>\n      <th>5001</th>\n      <td>0.207971804</td>\n    </tr>\n  </tbody>\n</table>\n<p>5002 rows × 1 columns</p>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Bidirectional, Input\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assuming 'data' is already defined and loaded\n# Ensure 'data' is a pandas Series\nif isinstance(data, pd.DataFrame):\n    data = data.iloc[:, 0]  # Select the first column or the relevant column\n\n# Handle non-numeric values\ndata = pd.to_numeric(data, errors='coerce')\ndata = data.dropna()\n\n# Reshape and scale the data\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n\n# Function to create sequences\ndef create_sequences(data, seq_length):\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length])\n    return np.array(X), np.array(y)\n\nseq_length = 8\nX, y = create_sequences(scaled_data, seq_length)\n\n# Reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Define model\nmodel = Sequential()\nmodel.add(Input(shape=(seq_length, n_features)))\nmodel.add(Bidirectional(LSTM(200, return_sequences=True, activation='relu')))\nmodel.add(Bidirectional(LSTM(100, return_sequences=True, activation='relu')))\nmodel.add(Bidirectional(LSTM(100, return_sequences=False, activation='relu')))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\n# Summarize model\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-07T14:22:02.088999Z","iopub.execute_input":"2024-11-07T14:22:02.089317Z","iopub.status.idle":"2024-11-07T14:22:02.28201Z","shell.execute_reply.started":"2024-11-07T14:22:02.089264Z","shell.execute_reply":"2024-11-07T14:22:02.281068Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ bidirectional_15                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m400\u001b[0m)         │       \u001b[38;5;34m323,200\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_16                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m200\u001b[0m)         │       \u001b[38;5;34m400,800\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_17                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │       \u001b[38;5;34m240,800\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m201\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ bidirectional_15                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">323,200</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_16                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">400,800</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_17                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">240,800</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m965,001\u001b[0m (3.68 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">965,001</span> (3.68 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m965,001\u001b[0m (3.68 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">965,001</span> (3.68 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Flatten the X_train and X_test\nX_train_flat = X_train.reshape(-1)\nX_test_flat = X_test.reshape(-1)\n\n# Flatten the y_train and y_test\ny_train_flat = y_train.reshape(-1)\ny_test_flat = y_test.reshape(-1)\n\n# Combine flattened X and y into single DataFrames\ntrain_df = pd.DataFrame({'Value': np.concatenate([X_train_flat, y_train_flat])})\ntest_df = pd.DataFrame({'Value': np.concatenate([X_test_flat, y_test_flat])})\n\n# Save to CSV files\ntrain_df.to_csv('train_data.csv', index=False)\ntest_df.to_csv('test_data.csv', index=False)\n\nprint(\"Training and testing data saved to CSV files.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T14:22:02.283176Z","iopub.execute_input":"2024-11-07T14:22:02.283454Z","iopub.status.idle":"2024-11-07T14:22:02.392213Z","shell.execute_reply.started":"2024-11-07T14:22:02.28343Z","shell.execute_reply":"2024-11-07T14:22:02.391301Z"}},"outputs":[{"name":"stdout","text":"Training and testing data saved to CSV files.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"history=model.fit(X, y, epochs=40, batch_size=1, validation_split=0.2)\nresults = model.evaluate(X_test, y_test, batch_size=1)\nprint(\"Test loss:\", results)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T14:22:02.394617Z","iopub.execute_input":"2024-11-07T14:22:02.395331Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 6ms/step - loss: 0.0086 - val_loss: 0.0022\nEpoch 2/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0022 - val_loss: 0.0019\nEpoch 3/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0016 - val_loss: 0.0015\nEpoch 4/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0017 - val_loss: 0.0015\nEpoch 5/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 0.0018\nEpoch 6/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 0.0019\nEpoch 7/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0013\nEpoch 8/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0012\nEpoch 9/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0015\nEpoch 10/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0011\nEpoch 11/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0011\nEpoch 12/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0011 - val_loss: 0.0014\nEpoch 13/40\n\u001b[1m3994/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0014\nEpoch 14/40\n\u001b[1m3619/3994\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0012","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Assuming you have already trained your LSTM model\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(X, y, batch_size=1)\nprint(\"Test loss, test accuracy:\", results)\n\n# Generate predictions (probabilities) on new data using `predict`\nprint(\"Generate predictions for 3 samples\")\npredictions = model.predict(X[:3])\nprint(\"Predictions shape:\", predictions.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Assuming you have already trained your LSTM model\n# Generate predictions for the entire dataset\npredictions = model.predict(X)\nprint(predictions)\n\n# Convert predictions to a DataFrame\npredictions_df = pd.DataFrame(predictions, columns=['Prediction'])\n\n# Save the DataFrame to a CSV file\npredictions_df.to_csv('predictions.csv', index=False)\n\nprint(\"Predictions saved to predictions.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on the training data\ntrain_predict = model.predict(X_train)\n\n# Predict on the testing data\ntest_predict = model.predict(X_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Predict on the training data\ntrain_predict = model.predict(X_train)\n\n# Predict on the testing data\ntest_predict = model.predict(X_test)\n\n# Plotting the results\nplt.figure(figsize=(14, 7))\n\n# Plot training data\n\nplt.plot(y_train, label='Actual')\n\nplt.title('Training Data')\nplt.xlabel('Time Steps')\nplt.ylabel('Values')\n\n\nplt.plot(train_predict, label='Predicted')\nplt.xlabel('Time Steps')\nplt.ylabel('Values')\n\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(y_test, label='Actual')\nplt.plot(test_predict, label='Predicted')\nplt.title('Testing Data')\nplt.xlabel('Time Steps')\nplt.ylabel('Values')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DataFrame with actual and predicted values\nresults_df = pd.DataFrame({\n    'Actual': y_test.flatten(),  # Flatten in case y_test is not 1D\n    'Predicted': test_predict.flatten()  # Flatten in case test_predict is not 1D\n})\n\n# Save the DataFrame to a CSV file\nresults_df.to_csv('test_vs_actual.csv', index=True)\n\nprint(\"Test vs Actual results saved to test_vs_actual.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n# make predictions\ny_pred = model.predict(X_test)\n\n# inverse transform predictions\ny_test_inv = scaler.inverse_transform(y_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\n\n# calculate metrics\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nmae = mean_absolute_error(y_test_inv, y_pred_inv)\nr2 = r2_score(y_test_inv, y_pred_inv)\n\nprint(f'MSE: {mse}')\nprint(f'MAE: {mae}')\nprint(f'R^2: {r2}')\n\n# plot predictions vs. actual values\nplt.figure(figsize=(10, 6))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.legend()\nplt.show()\nplt.savefig('predictions_vs_actual.png')\n\n# plot training and validation loss\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.show()\nplt.savefig('training_validation.png')\n# plot residuals\nresiduals = y_test_inv - y_pred_inv\nplt.figure(figsize=(10, 6))\nplt.plot(residuals)\nplt.title('Residuals')\nplt.show()\nplt.savefig('residual.png')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}